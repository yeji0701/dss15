{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'crawler', using template directory '/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/templates/project', created in:\r\n",
      "    /home/ubuntu/python3/notebook/Scrapy/crawler\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd crawler\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mcrawler\u001b[00m\r\n",
      "├── \u001b[01;34mcrawler\u001b[00m\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── items.py\r\n",
      "│   ├── middlewares.py\r\n",
      "│   ├── pipelines.py\r\n",
      "│   ├── settings.py\r\n",
      "│   └── \u001b[01;34mspiders\u001b[00m\r\n",
      "│       └── __init__.py\r\n",
      "└── scrapy.cfg\r\n",
      "\r\n",
      "2 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scrapy의 구조\n",
    "- spiders\n",
    "    - 어떤 웹 서비스를 어떻게 크롤링할 것인지에 대한 코드 작성 (.py로 작성)\n",
    "- items.py\n",
    "    - 모델에 해당하는 코드, 저장하는 데이터의 자료구조를 설정\n",
    "- pipelines.py\n",
    "    - scraping한 결과물을 item 형태로 구성하고 처리하는 방법에 대한 코드\n",
    "- settings.py\n",
    "    - scraping할 때의 환경 설정 값을 지정\n",
    "    - robots.txt 따를지 안따를지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. xpath 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get('http://corners.gmarket.co.kr/Bestsellers')\n",
    "response = TextResponse(req.url, body=req.text, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " 'http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400801055234986')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = response.xpath('//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li/div[1]/a/@href').extract()\n",
    "len(links), links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('20년 멋진밥상 흥양농협 햅쌀(단일품종) 20kg ', '63900', '59900', '106.68%')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req = requests.get(links[0])\n",
    "response = TextResponse(req.url, body=req.text, encoding='utf-8')\n",
    "title = response.xpath('//*[@id=\"itemcase_basic\"]/h1/text()')[0].extract()\n",
    "s_price = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
    "o_price = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/strong/text()')[0].extract().replace(\",\", \"\")\n",
    "discount_rate = str(round(int(s_price) / int(o_price) * 100, 2)) + '%'\n",
    "title, s_price, o_price, discount_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. items.py 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define here the models for your scraped items\r\n",
      "#\r\n",
      "# See documentation in:\r\n",
      "# https://docs.scrapy.org/en/latest/topics/items.html\r\n",
      "\r\n",
      "import scrapy\r\n",
      "\r\n",
      "\r\n",
      "class CrawlerItem(scrapy.Item):\r\n",
      "    # define the fields for your item here like:\r\n",
      "    # name = scrapy.Field()\r\n",
      "    pass\r\n"
     ]
    }
   ],
   "source": [
    "!cat crawler/crawler/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crawler/crawler/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler/crawler/items.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class CrawlerItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    s_price = scrapy.Field()\n",
    "    o_price = scrapy.Field()\n",
    "    discount_rate = scrapy.Field()\n",
    "    link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. spider.py 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing crawler/crawler/spiders/spiders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler/crawler/spiders/spiders.py\n",
    "import scrapy\n",
    "from crawler.items import CrawlerItem\n",
    "\n",
    "class Spider(scrapy.Spider):\n",
    "    name = 'GmarketBestsellers'\n",
    "    allow_domain = ['gmarket.co.kr']\n",
    "    start_urls = ['http://corners.gmarket.co.kr/Bestsellers']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        links = response.xpath('//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li/div[1]/a/@href').extract()\n",
    "        for link in links[:10]:\n",
    "            yield scrapy.Request(link, callback=self.page_content)\n",
    "    \n",
    "    def page_content(self, response):\n",
    "        item = CrawlerItem()\n",
    "        item['title'] = response.xpath('//*[@id=\"itemcase_basic\"]/h1/text()')[0].extract()\n",
    "        item['o_price'] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
    "        try:\n",
    "            item['s_price'] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/strong/text()')[0].extract().replace(\",\", \"\")\n",
    "        except:\n",
    "            item['o_price'] = item['s_price']\n",
    "        item['discount_rate'] = str(round((1 - int(item['s_price']) / int(item['o_price'])) * 100, 2)) + '%'\n",
    "        item['link'] = response.url\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Scrapy 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd crawler\n",
    "scrapy crawl GmarketBestsellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 09:48:38 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: crawler)\n",
      "2020-11-04 09:48:38 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Oct 16 2020, 03:09:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.2.1, Platform Linux-5.4.0-1029-aws-x86_64-with-debian-buster-sid\n",
      "2020-11-04 09:48:38 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-11-04 09:48:38 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'crawler',\n",
      " 'NEWSPIDER_MODULE': 'crawler.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['crawler.spiders']}\n",
      "2020-11-04 09:48:38 [scrapy.extensions.telnet] INFO: Telnet Password: d276b085c9a90599\n",
      "2020-11-04 09:48:38 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-11-04 09:48:38 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-04 09:48:38 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-04 09:48:38 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-04 09:48:38 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-04 09:48:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-04 09:48:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-04 09:48:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://corners.gmarket.co.kr/robots.txt> (referer: None)\n",
      "2020-11-04 09:48:38 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.\n",
      "2020-11-04 09:48:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://corners.gmarket.co.kr/Bestsellers> (referer: None)\n",
      "2020-11-04 09:48:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/robots.txt> (referer: None)\n",
      "2020-11-04 09:48:39 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.\n",
      "2020-11-04 09:48:39 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.\n",
      "2020-11-04 09:48:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1836443282&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1555983157&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:40 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1836443282&ver=637400801190359810>\n",
      "{'discount_rate': '3.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1836443282&ver=637400801190359810',\n",
      " 'o_price': '22000',\n",
      " 's_price': '21340',\n",
      " 'title': '하남 핫푸드  하남쭈꾸미 350g X3팩 '}\n",
      "2020-11-04 09:48:40 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1555983157&ver=637400801190359810>\n",
      "{'discount_rate': '20.11%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1555983157&ver=637400801190359810',\n",
      " 'o_price': '17400',\n",
      " 's_price': '13900',\n",
      " 'title': '[청정원] 스파게티/파스타소스 600gx3 +면500g x2 or파우치증정 '}\n",
      "2020-11-04 09:48:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1639345413&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:40 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1639345413&ver=637400801190359810>\n",
      "{'discount_rate': '50.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1639345413&ver=637400801190359810',\n",
      " 'o_price': '19800',\n",
      " 's_price': '9900',\n",
      " 'title': '양평해장국 600g X 3팩 /선봉식품/즉석국/탕/찌개 '}\n",
      "2020-11-04 09:48:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1741016466&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1836990861&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1919830300&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1741016466&ver=637400801190359810>\n",
      "{'discount_rate': '50.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1741016466&ver=637400801190359810',\n",
      " 'o_price': '55800',\n",
      " 's_price': '27900',\n",
      " 'title': '삼진어묵  옛날모듬어묵 1Kg x 3개 '}\n",
      "2020-11-04 09:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1836990861&ver=637400801190359810>\n",
      "{'discount_rate': '2.38%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1836990861&ver=637400801190359810',\n",
      " 'o_price': '58900',\n",
      " 's_price': '57500',\n",
      " 'title': 'Shop+ New 에코라믹 통주물 블루밍 냄비 세트 총 6종(냄비 4종+콤비팬 2종)+실 '}\n",
      "2020-11-04 09:48:41 [scrapy.core.scraper] ERROR: Spider error processing <GET http://item.gmarket.co.kr/Item?goodscode=1919830300&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/python3/notebook/Scrapy/crawler/crawler/spiders/spiders.py\", line 17, in page_content\n",
      "    item['o_price'] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/parsel/selector.py\", line 70, in __getitem__\n",
      "    o = super(SelectorList, self).__getitem__(pos)\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 09:48:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1926067512&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400801190359810>\n",
      "{'discount_rate': '6.26%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400801190359810',\n",
      " 'o_price': '63900',\n",
      " 's_price': '59900',\n",
      " 'title': '20년 멋진밥상 흥양농협 햅쌀(단일품종) 20kg '}\n",
      "2020-11-04 09:48:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1840147374&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1836449519&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1926067512&ver=637400801190359810>\n",
      "{'discount_rate': '50.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1926067512&ver=637400801190359810',\n",
      " 'o_price': '30800',\n",
      " 's_price': '15400',\n",
      " 'title': '만두 새롭게 돌아온 화제의 호떡쌀군만두1.2kg+1.2kg '}\n",
      "2020-11-04 09:48:41 [scrapy.core.scraper] ERROR: Spider error processing <GET http://item.gmarket.co.kr/Item?goodscode=1840147374&ver=637400801190359810> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/python3/notebook/Scrapy/crawler/crawler/spiders/spiders.py\", line 17, in page_content\n",
      "    item['o_price'] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/parsel/selector.py\", line 70, in __getitem__\n",
      "    o = super(SelectorList, self).__getitem__(pos)\n",
      "IndexError: list index out of range\n",
      "2020-11-04 09:48:41 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1836449519&ver=637400801190359810>\n",
      "{'discount_rate': '2.57%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1836449519&ver=637400801190359810',\n",
      " 'o_price': '29550',\n",
      " 's_price': '28790',\n",
      " 'title': '하남 핫푸드  하남쭈꾸미 500g X3팩 '}\n",
      "2020-11-04 09:48:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-04 09:48:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 4268,\n",
      " 'downloader/request_count': 13,\n",
      " 'downloader/request_method_count/GET': 13,\n",
      " 'downloader/response_bytes': 370902,\n",
      " 'downloader/response_count': 13,\n",
      " 'downloader/response_status_count/200': 13,\n",
      " 'elapsed_time_seconds': 2.39716,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 4, 0, 48, 41, 342190),\n",
      " 'item_scraped_count': 8,\n",
      " 'log_count/DEBUG': 24,\n",
      " 'log_count/ERROR': 2,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 54919168,\n",
      " 'memusage/startup': 54919168,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 13,\n",
      " 'robotstxt/request_count': 2,\n",
      " 'robotstxt/response_count': 2,\n",
      " 'robotstxt/response_status_count/200': 2,\n",
      " 'scheduler/dequeued': 11,\n",
      " 'scheduler/dequeued/memory': 11,\n",
      " 'scheduler/enqueued': 11,\n",
      " 'scheduler/enqueued/memory': 11,\n",
      " 'spider_exceptions/IndexError': 2,\n",
      " 'start_time': datetime.datetime(2020, 11, 4, 0, 48, 38, 945030)}\n",
      "2020-11-04 09:48:41 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!./run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 csv로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd crawler\n",
    "scrapy crawl GmarketBestsellers -o GmarketBestsellers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 09:48:49 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: crawler)\n",
      "2020-11-04 09:48:49 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Oct 16 2020, 03:09:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.2.1, Platform Linux-5.4.0-1029-aws-x86_64-with-debian-buster-sid\n",
      "2020-11-04 09:48:49 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-11-04 09:48:49 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'crawler',\n",
      " 'NEWSPIDER_MODULE': 'crawler.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['crawler.spiders']}\n",
      "2020-11-04 09:48:49 [scrapy.extensions.telnet] INFO: Telnet Password: c7d521fa228fc7b3\n",
      "2020-11-04 09:48:49 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-11-04 09:48:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-04 09:48:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-04 09:48:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-04 09:48:49 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-04 09:48:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-04 09:48:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-04 09:48:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://corners.gmarket.co.kr/robots.txt> (referer: None)\n",
      "2020-11-04 09:48:49 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.\n",
      "2020-11-04 09:48:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://corners.gmarket.co.kr/Bestsellers> (referer: None)\n",
      "2020-11-04 09:48:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/robots.txt> (referer: None)\n",
      "2020-11-04 09:48:50 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.\n",
      "2020-11-04 09:48:50 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.\n",
      "2020-11-04 09:48:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1555983157&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1836443282&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1555983157&ver=637400801300536319>\n",
      "{'discount_rate': '20.11%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1555983157&ver=637400801300536319',\n",
      " 'o_price': '17400',\n",
      " 's_price': '13900',\n",
      " 'title': '[청정원] 스파게티/파스타소스 600gx3 +면500g x2 or파우치증정 '}\n",
      "2020-11-04 09:48:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1836443282&ver=637400801300536319>\n",
      "{'discount_rate': '3.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1836443282&ver=637400801300536319',\n",
      " 'o_price': '22000',\n",
      " 's_price': '21340',\n",
      " 'title': '하남 핫푸드  하남쭈꾸미 350g X3팩 '}\n",
      "2020-11-04 09:48:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1836990861&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:51 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400801300536319>\n",
      "{'discount_rate': '6.26%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400801300536319',\n",
      " 'o_price': '63900',\n",
      " 's_price': '59900',\n",
      " 'title': '20년 멋진밥상 흥양농협 햅쌀(단일품종) 20kg '}\n",
      "2020-11-04 09:48:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1919830300&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1836990861&ver=637400801300536319>\n",
      "{'discount_rate': '2.38%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1836990861&ver=637400801300536319',\n",
      " 'o_price': '58900',\n",
      " 's_price': '57500',\n",
      " 'title': 'Shop+ New 에코라믹 통주물 블루밍 냄비 세트 총 6종(냄비 4종+콤비팬 2종)+실 '}\n",
      "2020-11-04 09:48:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1741016466&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://item.gmarket.co.kr/Item?goodscode=1919830300&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/python3/notebook/Scrapy/crawler/crawler/spiders/spiders.py\", line 17, in page_content\n",
      "    item['o_price'] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/parsel/selector.py\", line 70, in __getitem__\n",
      "    o = super(SelectorList, self).__getitem__(pos)\n",
      "IndexError: list index out of range\n",
      "2020-11-04 09:48:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1639345413&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 09:48:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1741016466&ver=637400801300536319>\n",
      "{'discount_rate': '50.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1741016466&ver=637400801300536319',\n",
      " 'o_price': '55800',\n",
      " 's_price': '27900',\n",
      " 'title': '삼진어묵  옛날모듬어묵 1Kg x 3개 '}\n",
      "2020-11-04 09:48:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1639345413&ver=637400801300536319>\n",
      "{'discount_rate': '50.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1639345413&ver=637400801300536319',\n",
      " 'o_price': '19800',\n",
      " 's_price': '9900',\n",
      " 'title': '양평해장국 600g X 3팩 /선봉식품/즉석국/탕/찌개 '}\n",
      "2020-11-04 09:48:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1836449519&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1926067512&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1836449519&ver=637400801300536319>\n",
      "{'discount_rate': '2.57%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1836449519&ver=637400801300536319',\n",
      " 'o_price': '29550',\n",
      " 's_price': '28790',\n",
      " 'title': '하남 핫푸드  하남쭈꾸미 500g X3팩 '}\n",
      "2020-11-04 09:48:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1840147374&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 09:48:52 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1926067512&ver=637400801300536319>\n",
      "{'discount_rate': '50.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1926067512&ver=637400801300536319',\n",
      " 'o_price': '30800',\n",
      " 's_price': '15400',\n",
      " 'title': '만두 새롭게 돌아온 화제의 호떡쌀군만두1.2kg+1.2kg '}\n",
      "2020-11-04 09:48:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://item.gmarket.co.kr/Item?goodscode=1840147374&ver=637400801300536319> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/python3/notebook/Scrapy/crawler/crawler/spiders/spiders.py\", line 17, in page_content\n",
      "    item['o_price'] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/parsel/selector.py\", line 70, in __getitem__\n",
      "    o = super(SelectorList, self).__getitem__(pos)\n",
      "IndexError: list index out of range\n",
      "2020-11-04 09:48:52 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-04 09:48:52 [scrapy.extensions.feedexport] INFO: Stored csv feed (8 items) in: GmarketBestsellers.csv\n",
      "2020-11-04 09:48:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 4268,\n",
      " 'downloader/request_count': 13,\n",
      " 'downloader/request_method_count/GET': 13,\n",
      " 'downloader/response_bytes': 371130,\n",
      " 'downloader/response_count': 13,\n",
      " 'downloader/response_status_count/200': 13,\n",
      " 'elapsed_time_seconds': 2.487038,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 4, 0, 48, 52, 435445),\n",
      " 'item_scraped_count': 8,\n",
      " 'log_count/DEBUG': 24,\n",
      " 'log_count/ERROR': 2,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 55279616,\n",
      " 'memusage/startup': 55279616,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 13,\n",
      " 'robotstxt/request_count': 2,\n",
      " 'robotstxt/response_count': 2,\n",
      " 'robotstxt/response_status_count/200': 2,\n",
      " 'scheduler/dequeued': 11,\n",
      " 'scheduler/dequeued/memory': 11,\n",
      " 'scheduler/enqueued': 11,\n",
      " 'scheduler/enqueued/memory': 11,\n",
      " 'spider_exceptions/IndexError': 2,\n",
      " 'start_time': datetime.datetime(2020, 11, 4, 0, 48, 49, 948407)}\n",
      "2020-11-04 09:48:52 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!./run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GmarketBestsellers.csv\tcrawler  scrapy.cfg\r\n"
     ]
    }
   ],
   "source": [
    "!ls crawler/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GmarketBestsellers.csv', 'crawler', 'scrapy.cfg']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = !ls crawler/\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of   discount_rate                                               link  o_price  \\\n",
       "0        20.11%  http://item.gmarket.co.kr/Item?goodscode=15559...    17400   \n",
       "1          3.0%  http://item.gmarket.co.kr/Item?goodscode=18364...    22000   \n",
       "2         6.26%  http://item.gmarket.co.kr/Item?goodscode=19307...    63900   \n",
       "3         2.38%  http://item.gmarket.co.kr/Item?goodscode=18369...    58900   \n",
       "4         50.0%  http://item.gmarket.co.kr/Item?goodscode=17410...    55800   \n",
       "5         50.0%  http://item.gmarket.co.kr/Item?goodscode=16393...    19800   \n",
       "6         2.57%  http://item.gmarket.co.kr/Item?goodscode=18364...    29550   \n",
       "7         50.0%  http://item.gmarket.co.kr/Item?goodscode=19260...    30800   \n",
       "\n",
       "   s_price                                              title  \n",
       "0    13900         [청정원] 스파게티/파스타소스 600gx3 +면500g x2 or파우치증정   \n",
       "1    21340                            하남 핫푸드  하남쭈꾸미 350g X3팩   \n",
       "2    59900                       20년 멋진밥상 흥양농협 햅쌀(단일품종) 20kg   \n",
       "3    57500  Shop+ New 에코라믹 통주물 블루밍 냄비 세트 총 6종(냄비 4종+콤비팬 2종...  \n",
       "4    27900                             삼진어묵  옛날모듬어묵 1Kg x 3개   \n",
       "5     9900                    양평해장국 600g X 3팩 /선봉식품/즉석국/탕/찌개   \n",
       "6    28790                            하남 핫푸드  하남쭈꾸미 500g X3팩   \n",
       "7    15400                  만두 새롭게 돌아온 화제의 호떡쌀군만두1.2kg+1.2kg   >"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"crawler/{}\".format(files[0]))\n",
    "df.tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Pipeline 설정\n",
    "- item을 설정하기 전에 실행되는 코드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_slack(msg):\n",
    "    WEBHOOK_URL = 'https://hooks.slack.com/services/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'\n",
    "    payload = {\n",
    "        \"channel\" : \"일반\",\n",
    "        \"username\" : \"YJ\",\n",
    "        \"text\" : msg,\n",
    "    }\n",
    "    requests.post(WEBHOOK_URL, json.dumps(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_slack('테스트')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define your item pipelines here\r\n",
      "#\r\n",
      "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\r\n",
      "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\r\n",
      "\r\n",
      "\r\n",
      "# useful for handling different item types with a single interface\r\n",
      "from itemadapter import ItemAdapter\r\n",
      "\r\n",
      "\r\n",
      "class CrawlerPipeline:\r\n",
      "    def process_item(self, item, spider):\r\n",
      "        return item\r\n"
     ]
    }
   ],
   "source": [
    "!cat crawler/crawler/pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crawler/crawler/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler/crawler/pipelines.py\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class CrawlerPipeline(object):\n",
    "    \n",
    "    def __send_slack(self, msg):\n",
    "        WEBHOOK_URL = 'https://hooks.slack.com/services/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'\n",
    "        payload = {\n",
    "        \"channel\" : \"일반\",\n",
    "        \"username\" : \"YJ\",\n",
    "        \"text\" : msg,\n",
    "        }\n",
    "        requests.post(WEBHOOK_URL, json.dumps(payload))\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        keyword = 'g'\n",
    "        print(\"=\"*100)\n",
    "        print(item[\"title\"], keyword)\n",
    "        print(\"=\"*100)\n",
    "        if keyword in item[\"title\"]:\n",
    "            self.__send_slack(\"{},{},{}\".format(\n",
    "                item[\"title\"], item[\"s_price\"], item[\"link\"]))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline 설정 : settings.py\n",
    "\n",
    "# ```\n",
    "# ITEM_PIPELINES = {\n",
    "#     'crawler.pipelines.CrawlerPipeline' : 300,\n",
    "# }\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"ITEM_PIPELINES = {\" >> crawler/crawler/settings.py\n",
    "!echo \"    'crawler.pipelines.CrawlerPipeline' : 300,\" >> crawler/crawler/settings.py\n",
    "!echo \"}\" >> crawler/crawler/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEM_PIPELINES = {\r\n",
      "    'crawler.pipelines.CrawlerPipeline' : 300,\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 3 crawler/crawler/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 10:15:44 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: crawler)\n",
      "2020-11-04 10:15:44 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Oct 16 2020, 03:09:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.2.1, Platform Linux-5.4.0-1029-aws-x86_64-with-debian-buster-sid\n",
      "2020-11-04 10:15:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-11-04 10:15:44 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'crawler',\n",
      " 'NEWSPIDER_MODULE': 'crawler.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['crawler.spiders']}\n",
      "2020-11-04 10:15:44 [scrapy.extensions.telnet] INFO: Telnet Password: 4eeaa26dda647c82\n",
      "2020-11-04 10:15:44 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-11-04 10:15:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-04 10:15:44 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-04 10:15:44 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "['crawler.pipelines.CrawlerPipeline']\n",
      "2020-11-04 10:15:44 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-04 10:15:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-04 10:15:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-04 10:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://corners.gmarket.co.kr/robots.txt> (referer: None)\n",
      "2020-11-04 10:15:44 [protego] DEBUG: Rule at line 3 without any user agent to enforce it on.\n",
      "2020-11-04 10:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://corners.gmarket.co.kr/Bestsellers> (referer: None)\n",
      "2020-11-04 10:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/robots.txt> (referer: None)\n",
      "2020-11-04 10:15:44 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.\n",
      "2020-11-04 10:15:44 [protego] DEBUG: Rule at line 2 without any user agent to enforce it on.\n",
      "2020-11-04 10:15:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1923682561&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 10:15:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1836443282&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 10:15:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1681223122&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "====================================================================================================\n",
      "KF94 황사방역 에코마스크  (새부리형/100매)  g\n",
      "====================================================================================================\n",
      "2020-11-04 10:15:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1923682561&ver=637400817447778258>\n",
      "{'discount_rate': '2.6%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1923682561&ver=637400817447778258',\n",
      " 'o_price': '50000',\n",
      " 's_price': '48700',\n",
      " 'title': 'KF94 황사방역 에코마스크  (새부리형/100매) '}\n",
      "====================================================================================================\n",
      "하남 핫푸드  하남쭈꾸미 350g X3팩  g\n",
      "====================================================================================================\n",
      "2020-11-04 10:15:46 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): hooks.slack.com:443\n",
      "2020-11-04 10:15:46 [urllib3.connectionpool] DEBUG: https://hooks.slack.com:443 \"POST /services/T01DG7CV88Z/B01EBGWHE2V/eiAttgyYje93Q21oQPkyWesQ HTTP/1.1\" 200 22\n",
      "2020-11-04 10:15:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1836443282&ver=637400817447778258>\n",
      "{'discount_rate': '3.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1836443282&ver=637400817447778258',\n",
      " 'o_price': '22000',\n",
      " 's_price': '21340',\n",
      " 'title': '하남 핫푸드  하남쭈꾸미 350g X3팩 '}\n",
      "====================================================================================================\n",
      "[브라운] 브라운 전기면도기 시리즈3 300TS  g\n",
      "====================================================================================================\n",
      "2020-11-04 10:15:46 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1681223122&ver=637400817447778258>\n",
      "{'discount_rate': '8.44%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1681223122&ver=637400817447778258',\n",
      " 'o_price': '45000',\n",
      " 's_price': '41200',\n",
      " 'title': '[브라운] 브라운 전기면도기 시리즈3 300TS '}\n",
      "2020-11-04 10:15:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1919830300&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 10:15:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 10:15:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://item.gmarket.co.kr/Item?goodscode=1919830300&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/python3/notebook/Scrapy/crawler/crawler/spiders/spiders.py\", line 17, in page_content\n",
      "    item['o_price'] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/parsel/selector.py\", line 70, in __getitem__\n",
      "    o = super(SelectorList, self).__getitem__(pos)\n",
      "IndexError: list index out of range\n",
      "====================================================================================================\n",
      "20년 멋진밥상 흥양농협 햅쌀(단일품종) 20kg  g\n",
      "====================================================================================================\n",
      "2020-11-04 10:15:46 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): hooks.slack.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 10:15:47 [urllib3.connectionpool] DEBUG: https://hooks.slack.com:443 \"POST /services/T01DG7CV88Z/B01EBGWHE2V/eiAttgyYje93Q21oQPkyWesQ HTTP/1.1\" 200 22\n",
      "2020-11-04 10:15:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400817447778258>\n",
      "{'discount_rate': '6.26%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1930786012&ver=637400817447778258',\n",
      " 'o_price': '63900',\n",
      " 's_price': '59900',\n",
      " 'title': '20년 멋진밥상 흥양농협 햅쌀(단일품종) 20kg '}\n",
      "2020-11-04 10:15:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1926067512&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 10:15:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1639345413&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 10:15:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1840147374&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 10:15:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1836449519&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "====================================================================================================\n",
      "만두 새롭게 돌아온 화제의 호떡쌀군만두1.2kg+1.2kg  g\n",
      "====================================================================================================\n",
      "2020-11-04 10:15:47 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): hooks.slack.com:443\n",
      "2020-11-04 10:15:47 [urllib3.connectionpool] DEBUG: https://hooks.slack.com:443 \"POST /services/T01DG7CV88Z/B01EBGWHE2V/eiAttgyYje93Q21oQPkyWesQ HTTP/1.1\" 200 22\n",
      "2020-11-04 10:15:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1926067512&ver=637400817447778258>\n",
      "{'discount_rate': '50.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1926067512&ver=637400817447778258',\n",
      " 'o_price': '30800',\n",
      " 's_price': '15400',\n",
      " 'title': '만두 새롭게 돌아온 화제의 호떡쌀군만두1.2kg+1.2kg '}\n",
      "====================================================================================================\n",
      "양평해장국 600g X 3팩 /선봉식품/즉석국/탕/찌개  g\n",
      "====================================================================================================\n",
      "2020-11-04 10:15:47 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): hooks.slack.com:443\n",
      "2020-11-04 10:15:48 [urllib3.connectionpool] DEBUG: https://hooks.slack.com:443 \"POST /services/T01DG7CV88Z/B01EBGWHE2V/eiAttgyYje93Q21oQPkyWesQ HTTP/1.1\" 200 22\n",
      "2020-11-04 10:15:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1639345413&ver=637400817447778258>\n",
      "{'discount_rate': '50.0%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1639345413&ver=637400817447778258',\n",
      " 'o_price': '19800',\n",
      " 's_price': '9900',\n",
      " 'title': '양평해장국 600g X 3팩 /선봉식품/즉석국/탕/찌개 '}\n",
      "2020-11-04 10:15:48 [scrapy.core.scraper] ERROR: Spider error processing <GET http://item.gmarket.co.kr/Item?goodscode=1840147374&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/python3/notebook/Scrapy/crawler/crawler/spiders/spiders.py\", line 17, in page_content\n",
      "    item['o_price'] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/parsel/selector.py\", line 70, in __getitem__\n",
      "    o = super(SelectorList, self).__getitem__(pos)\n",
      "IndexError: list index out of range\n",
      "====================================================================================================\n",
      "하남 핫푸드  하남쭈꾸미 500g X3팩  g\n",
      "====================================================================================================\n",
      "2020-11-04 10:15:48 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): hooks.slack.com:443\n",
      "2020-11-04 10:15:48 [urllib3.connectionpool] DEBUG: https://hooks.slack.com:443 \"POST /services/T01DG7CV88Z/B01EBGWHE2V/eiAttgyYje93Q21oQPkyWesQ HTTP/1.1\" 200 22\n",
      "2020-11-04 10:15:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://item.gmarket.co.kr/Item?goodscode=1836449519&ver=637400817447778258>\n",
      "{'discount_rate': '2.57%',\n",
      " 'link': 'http://item.gmarket.co.kr/Item?goodscode=1836449519&ver=637400817447778258',\n",
      " 'o_price': '29550',\n",
      " 's_price': '28790',\n",
      " 'title': '하남 핫푸드  하남쭈꾸미 500g X3팩 '}\n",
      "2020-11-04 10:15:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://item.gmarket.co.kr/Item?goodscode=1927444856&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "2020-11-04 10:15:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://item.gmarket.co.kr/Item?goodscode=1927444856&ver=637400817447778258> (referer: http://corners.gmarket.co.kr/Bestsellers)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/python3/notebook/Scrapy/crawler/crawler/spiders/spiders.py\", line 17, in page_content\n",
      "    item['o_price'] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/parsel/selector.py\", line 70, in __getitem__\n",
      "    o = super(SelectorList, self).__getitem__(pos)\n",
      "IndexError: list index out of range\n",
      "2020-11-04 10:15:49 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-04 10:15:49 [scrapy.extensions.feedexport] INFO: Stored csv feed (7 items) in: GmarketBestsellers.csv\n",
      "2020-11-04 10:15:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 4268,\n",
      " 'downloader/request_count': 13,\n",
      " 'downloader/request_method_count/GET': 13,\n",
      " 'downloader/response_bytes': 387344,\n",
      " 'downloader/response_count': 13,\n",
      " 'downloader/response_status_count/200': 13,\n",
      " 'elapsed_time_seconds': 4.495789,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 4, 1, 15, 49, 115404),\n",
      " 'item_scraped_count': 7,\n",
      " 'log_count/DEBUG': 33,\n",
      " 'log_count/ERROR': 3,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 58499072,\n",
      " 'memusage/startup': 58499072,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 13,\n",
      " 'robotstxt/request_count': 2,\n",
      " 'robotstxt/response_count': 2,\n",
      " 'robotstxt/response_status_count/200': 2,\n",
      " 'scheduler/dequeued': 11,\n",
      " 'scheduler/dequeued/memory': 11,\n",
      " 'scheduler/enqueued': 11,\n",
      " 'scheduler/enqueued/memory': 11,\n",
      " 'spider_exceptions/IndexError': 3,\n",
      " 'start_time': datetime.datetime(2020, 11, 4, 1, 15, 44, 619615)}\n",
      "2020-11-04 10:15:49 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!./run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
